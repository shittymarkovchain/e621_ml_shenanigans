{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cKirMSQUkwna"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "from apex import amp\n",
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oZ91CH-Ik2QK"
   },
   "outputs": [],
   "source": [
    "ROOT = 'D:/data/stuff'\n",
    "OUT = f'{ROOT}/fav_1'\n",
    "\n",
    "if not Path(OUT).exists():\n",
    "    ! mkdir \"$OUT\"\n",
    "    ! mkdir \"$OUT/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zj0U94NY6Tan"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1762920"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{ROOT}/updated_posts.csv\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "printable = set(string.printable)\n",
    "df[\"tags\"] = df[\"tags\"].apply(lambda s: ''.join(filter(lambda x: x in printable, s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZPX8eVfSoaQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148259"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_read_csv = False\n",
    "if re_read_csv:\n",
    "    all_posts = set(df.id.values)\n",
    "    favs = pd.read_csv(f\"{ROOT}/favs.csv\")\n",
    "    user_favs = defaultdict(set)\n",
    "    for post, user in favs.values:\n",
    "        if post in all_posts:\n",
    "            user_favs[user].add(post)\n",
    "    del all_posts\n",
    "    tmp = pd.DataFrame([(x, user_favs[x]) for x in user_favs])\n",
    "    tmp.to_csv(f\"{ROOT}/favs_grouped.csv\")\n",
    "    del tmp\n",
    "else:\n",
    "    tmp = pd.read_csv(f\"{ROOT}/favs_grouped.csv\")\n",
    "    user_favs = dict()\n",
    "    for _, u, f in tmp.values:\n",
    "        user_favs[u] = set([int(x) for x in f[1:-1].split(\", \")])\n",
    "    del tmp\n",
    "len(user_favs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rO5lQLoH6ZeP"
   },
   "outputs": [],
   "source": [
    "tags_count = defaultdict(lambda: 0)\n",
    "for tags in df[\"tags\"]:\n",
    "    for t in tags.split():\n",
    "        tags_count[t] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "l8pcUCTyxXGO",
    "outputId": "d68dc7e6-bc50-441c-c50b-bde60ddc259a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "re_read_tags = False\n",
    "if re_read_tags:\n",
    "    sorted_tags = [(-tags_count[x], x) for x in tags_count]\n",
    "    sorted_tags.sort()\n",
    "    common_tags = [(x, -n) for n, x in sorted_tags]\n",
    "    with open(f\"{ROOT}/common_tags.csv\", \"w\") as f:\n",
    "        for t, n in common_tags:\n",
    "            print(f\"{t},{n}\", file=f)\n",
    "else:\n",
    "    common_tags = pd.read_csv(f\"{ROOT}/common_tags.csv\", header=None).values\n",
    "    \n",
    "\n",
    "n_tags = 3 * 1024\n",
    "common_tags = [x for x, _ in common_tags[:n_tags]]\n",
    "len(common_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "dxm3TDNM7As_",
    "outputId": "088415d0-7d1d-4fb3-9901-184955dce1e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25956624, 0.4489894 , 0.53869315, ..., 7.42023862, 7.42308357,\n",
       "       7.42308357])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = np.array([len(df) / tags_count[t] for t in common_tags])\n",
    "normalizer = np.log(normalizer)\n",
    "normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjFVyrblk_so"
   },
   "outputs": [],
   "source": [
    "reduced_components = 256\n",
    "\n",
    "regen_data = True\n",
    "data_preprocess_path = f\"{ROOT}/data_preprocess\"\n",
    "\n",
    "if regen_data:\n",
    "\n",
    "    posts_mapping = dict()\n",
    "    posts_encoded = []\n",
    "\n",
    "    for df in pd.read_csv(f\"{ROOT}/data_all.csv\", chunksize=4096):\n",
    "        df = df.drop(columns=[\"Unnamed: 0\", \"Unnamed: 0.1\", \"author\", \"file_url\", \"sample_url\"])\n",
    "        df[\"tags\"] = df[\"tags\"].apply(lambda x: set(x.split()))\n",
    "        for fav, id, rating, score, tags in df.values:\n",
    "\n",
    "            tags = np.array([1 if t in tags else 0 for t in common_tags]) * normalizer\n",
    "            res = [score, fav]\n",
    "            res += [1 if r == rating else 0 for r in ['s', 'q', 'e']]\n",
    "            res += tags.tolist()\n",
    "            posts_mapping[id] = len(posts_encoded)\n",
    "            posts_encoded.append(np.array(res))\n",
    "    all_ids = list(posts_mapping)\n",
    "    \n",
    "    pca = sklearn.decomposition.PCA(n_components=reduced_components - 5, whiten=True)\n",
    "    #scaler = StandardScaler()\n",
    "    X = np.array(np.array(posts_encoded)[:, 5:])\n",
    "    #X = scaler.fit_transform(X)\n",
    "    X = pca.fit_transform(X)\n",
    "    posts_encoded = np.concatenate([np.array(posts_encoded)[:, :5], X], axis=1)\n",
    "    \n",
    "    \n",
    "    pickle.dump([posts_mapping, posts_encoded, all_ids, pca], open(data_preprocess_path, \"wb\"), protocol=4)\n",
    "else:\n",
    "    posts_mapping, posts_encoded, all_ids, _ = pickle.load(open(data_preprocess_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "thpJBDKJmlNi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GtJVXUbfwaPQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUiyIvrZuCX1"
   },
   "outputs": [],
   "source": [
    "def convert(post_id):\n",
    "    return posts_encoded[posts_mapping[post_id]]\n",
    "def convert_list(l):\n",
    "    for x in l:\n",
    "        try:\n",
    "            yield(convert(x))\n",
    "        except:\n",
    "            pass\n",
    "def get_n_posts(n, user_id):\n",
    "    return [convert(x) for x in random.sample(user_favs[user_id], n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_sizes = [64, 128, 256, 512, 1024]\n",
    "\n",
    "regenerate_valid_users = False\n",
    "if regenerate_valid_users:\n",
    "    valid_users_dict = dict()\n",
    "    for s in group_sizes:\n",
    "        res = set()\n",
    "        for u in user_favs:\n",
    "            if len(list(get_n_posts(s, u))) == s:\n",
    "                res.add(u)\n",
    "        valid_users_dict[s] = res\n",
    "    pickle.dump(valid_users_dict, open(f\"{ROOT}/valid_users.p\", \"wb\"))\n",
    "else:\n",
    "    valid_users_dict = pickle.load(open(f\"{ROOT}/valid_users.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wV0xwKjtE5NG"
   },
   "outputs": [],
   "source": [
    "def save_all(model, opti, n_batch):\n",
    "    out_path = f\"{OUT}/all_train_data\"\n",
    "    data = [model.state_dict(), opti.state_dict(), n_batch]\n",
    "    out_path_2 = f\"{OUT}/train_{(n_batch // 5000) % 50}\"\n",
    "    torch.save(data, open(out_path, \"wb\"))\n",
    "    ! cp \"$out_path\" \"$out_path_2\"\n",
    "def load_all(model, opti):\n",
    "    in_path = f\"{OUT}/all_train_data\"\n",
    "    l = torch.load(open(in_path, \"rb\"))\n",
    "    model.load_state_dict(l[0])\n",
    "    opti.load_state_dict(l[1])\n",
    "    return [model, opti] + l[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jpfiyGrHkmue",
    "outputId": "8e01152f-a473-4d34-9e90-e567b8efd00c"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMpsxyOontFk"
   },
   "outputs": [],
   "source": [
    "def layer(in_depth, k):\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Conv1d(in_depth, k, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "        torch.nn.BatchNorm1d(k),\n",
    "        torch.nn.LeakyReLU(0.2)\n",
    "    )\n",
    "size_bottleneck = 64\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.conv = torch.nn.ModuleList([\n",
    "            layer(reduced_components, 1024),\n",
    "            layer(1024, 512),\n",
    "        ])\n",
    "        \n",
    "        self.features_extract = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(512 * 2 + reduced_components, 1024),\n",
    "            torch.nn.LeakyReLU(0.2),\n",
    "            torch.nn.Linear(1024, 256),\n",
    "            torch.nn.LeakyReLU(0.2),\n",
    "            torch.nn.Linear(256, size_bottleneck),\n",
    "        ])\n",
    "        \n",
    "        self.is_fav = torch.nn.ModuleList([\n",
    "            layer(reduced_components + size_bottleneck, 1024),\n",
    "            layer(1024, 512),\n",
    "            layer(512, 128),\n",
    "            layer(128, 128),\n",
    "            layer(128, 32),\n",
    "            torch.nn.Conv1d(32, 1, kernel_size=1, stride=1, padding=0),\n",
    "            torch.nn.Sigmoid()\n",
    "        ])\n",
    "    \n",
    "    def forward(self, favs, to_identify):\n",
    "        #print(favs.mean())\n",
    "        x = favs\n",
    "        for l in self.conv:\n",
    "            x = l(x)\n",
    "        x_mean = x.mean(dim=2)\n",
    "        x_var = x.var(dim=2)\n",
    "        inputs_mean = favs.mean(dim=2)\n",
    "        x = torch.cat([x_mean, x_var, inputs_mean], dim=1)\n",
    "        for l in self.features_extract:\n",
    "            x = l(x)\n",
    "            #print(l, x)\n",
    "        \n",
    "        fingerprint = x\n",
    "        \n",
    "        user_extended = x.view(-1, size_bottleneck, 1).repeat(1, 1, to_identify.shape[-1])\n",
    "        x = torch.cat([to_identify, user_extended], dim=1)\n",
    "        \n",
    "        for l in self.is_fav:\n",
    "            x = l(x)\n",
    "        return x.reshape(x.shape[0], -1), fingerprint\n",
    "model = Network().to(device)\n",
    "\n",
    "\n",
    "\n",
    "lr = 0.000005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss = torch.nn.BCELoss()\n",
    "MP = False\n",
    "\n",
    "if MP:\n",
    "    model, optimizer = amp.initialize(\n",
    "       model, optimizer, opt_level=\"O2\"\n",
    "    )\n",
    "\n",
    "n_batch = 0\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "moQBirTN85di"
   },
   "outputs": [],
   "source": [
    "model, optimizer, n_batch = load_all(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "pOjhHgZ7yl9C",
    "outputId": "dad2c12f-984f-424e-cac8-1684f6f3ccd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111900 : 0.6220861673355103, positive : 0.95782470703125, negative : 0.3804931640625, diff : 0.02625551074743271, variance : 2.7482106685638428\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-fac48b536069>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mextract1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0mextract2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accumulate_steps = 1\n",
    "model.train()\n",
    "for epoch in range(100000000):\n",
    "    n_samples = random.choice(group_sizes) // 2\n",
    "    batch_size = int((512 / n_samples) * 32)\n",
    "    ones = torch.ones(batch_size).to(device)\n",
    "    valid_users = list(valid_users_dict[n_samples * 2])\n",
    "\n",
    "    n2_samples = n_samples * 2\n",
    "    random.shuffle(valid_users)\n",
    "    #print(valid_users)\n",
    "\n",
    "    #print(n_batch)\n",
    "    n_batch += 1\n",
    "\n",
    "    extract1 = []\n",
    "    extract2 = []\n",
    "    inputs_test1 = []\n",
    "    inputs_test2 = []\n",
    "\n",
    "    for u in random.sample(valid_users, batch_size):\n",
    "        favs = list(get_n_posts(n2_samples, u))\n",
    "\n",
    "        extract1.append(np.array(favs[:n_samples]).T)\n",
    "        extract2.append(np.array(favs[n_samples:n2_samples]).T)\n",
    "\n",
    "    targets = torch.tensor([[1] * n_samples + [0] * n_samples] * batch_size, device=device, dtype=torch.float)\n",
    "    \n",
    "    weights = targets * 2 + 1\n",
    "    weights = weights / weights.mean()\n",
    "    loss = torch.nn.BCELoss(weight=weights)\n",
    "    \n",
    "    extract1 = torch.tensor(extract1, device=device, dtype=torch.float)\n",
    "    extract2 = torch.tensor(extract2, device=device, dtype=torch.float)\n",
    "    \n",
    "    to_test1 = torch.cat([extract2, extract2.roll(1, dims=[0])], dim=2)\n",
    "    to_test2 = torch.cat([extract1, extract1.roll(1, dims=[0])], dim=2)\n",
    "\n",
    "    res1, fingerprint1 = model(extract1, to_test1)\n",
    "    res2, fingerprint2 = model(extract2, to_test2)\n",
    "\n",
    "    loss1 = loss(res1, targets)\n",
    "    loss2 = loss(res2, targets)\n",
    "    loss_diff = ((fingerprint1 - fingerprint2) ** 2).mean()\n",
    "    variance = fingerprint1.var(dim=0).mean() + fingerprint2.var(dim=0).mean()\n",
    "    norm_loss = (fingerprint1 ** 2).mean() + (fingerprint2 ** 2).mean()\n",
    "    loss_variance = -variance\n",
    "    both_loss = loss1 + loss2\n",
    "    l = both_loss + loss_diff ** 2 * 0.002\n",
    "\n",
    "\n",
    "    assert(l == l)\n",
    "\n",
    "    l = l / accumulate_steps\n",
    "    if MP:\n",
    "        with amp.scale_loss(l, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "    else:\n",
    "        l.backward()\n",
    "        \n",
    "\n",
    "    if n_batch % accumulate_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    if n_batch % 1 == 0:\n",
    "        losses.append(l.item() * accumulate_steps)\n",
    "    if n_batch % 100 == 0:\n",
    "        mean_positive = (res1[:, :n_samples] > 0.5).float().mean().item()\n",
    "        mean_negative = (res1[:, n_samples:] > 0.5).float().mean().item()\n",
    "        print(f\"{n_batch} : {both_loss.item()}, positive : {mean_positive}, negative : {mean_negative}, diff : {loss_diff.item()}, variance : {variance.item()}\")\n",
    "    if n_batch % 5000 == 0:\n",
    "        save_all(model, optimizer, n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GSsv_HJbUGtW"
   },
   "outputs": [],
   "source": [
    "save_all(model, optimizer, n_batch)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ml_favs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
